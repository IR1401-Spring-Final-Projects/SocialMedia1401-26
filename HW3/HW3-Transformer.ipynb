{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48514f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import json\n",
    "import unidecode \n",
    "import re \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1518d7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science.xlsx: (2080, 19)\n",
      "JoeBiden.xlsx: (2066, 19)\n",
      "trump.xlsx: (2080, 19)\n",
      "nvidia.xlsx: (2010, 19)\n",
      "Meta.xlsx: (1644, 19)\n",
      "ISS.xlsx: (2053, 19)\n",
      "Google.xlsx: (2051, 19)\n",
      "robots.xlsx: (1982, 19)\n",
      "Twitter.xlsx: (1616, 19)\n",
      "abortion.xlsx: (2080, 19)\n",
      "IranPolitics.xlsx: (2080, 19)\n",
      "elon.xlsx: (2010, 19)\n",
      "metaverse.xlsx: (1553, 19)\n",
      "nasa.xlsx: (2080, 19)\n",
      "workers.xlsx: (2079, 19)\n",
      "Microsoft.xlsx: (2080, 19)\n",
      "democrats.xlsx: (2072, 19)\n",
      "facebook.xlsx: (2027, 19)\n",
      "developers.xlsx: (2080, 19)\n",
      "AbortionBan.xlsx: (2080, 19)\n",
      "ukraine.xlsx: (2080, 19)\n",
      "spaceX.xlsx: (2010, 19)\n",
      "WWDC.xlsx: (2080, 19)\n",
      "tech.xlsx: (2076, 19)\n",
      "instagram.xlsx: (1949, 19)\n",
      "Amazon.xlsx: (2078, 19)\n",
      "Apple.xlsx: (2031, 19)\n",
      "war.xlsx: (2079, 19)\n",
      "AbortionRightsAreHumanRights.xlsx: (2080, 19)\n",
      "rocket.xlsx: (2056, 19)\n",
      "Tesla.xlsx: (2004, 19)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for file in os.listdir(\"./Excels\"):\n",
    "    df = pd.read_excel(os.path.join(\"./Excels\", file))\n",
    "    dfs.append(df)\n",
    "    print(file, end=\": \")\n",
    "    print(df.shape)\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21431d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24116 entries, 0 to 24115\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Tweet Id     24116 non-null  int64 \n",
      " 1   Text         24116 non-null  object\n",
      " 2   Name         24115 non-null  object\n",
      " 3   Screen Name  24116 non-null  object\n",
      " 4   UTC          24116 non-null  object\n",
      " 5   Created At   24116 non-null  object\n",
      " 6   Favorites    24116 non-null  int64 \n",
      " 7   Retweets     24116 non-null  int64 \n",
      " 8   Language     24116 non-null  object\n",
      " 9   Client       24116 non-null  object\n",
      " 10  Tweet Type   24116 non-null  object\n",
      " 11  URLs         10973 non-null  object\n",
      " 12  Hashtags     24116 non-null  int64 \n",
      " 13  Mentions     24116 non-null  int64 \n",
      " 14  Media Type   8240 non-null   object\n",
      " 15  Media URLs   8240 non-null   object\n",
      " 16  Unnamed: 16  913 non-null    object\n",
      " 17  Unnamed: 17  546 non-null    object\n",
      " 18  Unnamed: 18  340 non-null    object\n",
      "dtypes: int64(5), object(14)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df[df['Language'] == 'en']\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop_duplicates(subset=\"Text\", keep='first', inplace=True, ignore_index=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0054ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "    \n",
    "        contraction_map_path = 'HW4/contraction_map.json'\n",
    "        # contraction_map_path = '/content/gdrive/MyDrive/MIR-HW4/contraction_map.json'\n",
    "        with open(contraction_map_path) as f:\n",
    "            self.contraction_map = json.load(f)\n",
    "        \n",
    "        nltk.download('stopwords')\n",
    "        self.stoplist = stopwords.words('english') \n",
    "        self.stoplist = set(self.stoplist)\n",
    "        \n",
    "        self.w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        \n",
    "    def remove_newlines_tabs(self, text):\n",
    "        formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ')\n",
    "        formatted_text = formatted_text.replace('\\\\', ' ').replace('. com', '.com')\n",
    "        return formatted_text\n",
    "\n",
    "    def remove_mentions_hashtagSigns(self, text):\n",
    "        tokenized = text.split(\" \")\n",
    "        formatted_text = \"\"\n",
    "        for w in tokenized:\n",
    "            if not w:\n",
    "                continue\n",
    "            if w[0] == \"@\":\n",
    "                continue\n",
    "            if w[0] == \"#\":\n",
    "                formatted_text += f\"{w[1:]} \"\n",
    "            else:\n",
    "                formatted_text += f\"{w} \"\n",
    "\n",
    "        return formatted_text.strip()\n",
    "\n",
    "    def strip_html_tags(self, text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        stripped_text = soup.get_text(separator=\" \")\n",
    "        return stripped_text\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        formatted = \"\"\n",
    "        for char in text:\n",
    "            if char in string.punctuation:\n",
    "                continue\n",
    "            else:\n",
    "                formatted += char\n",
    "        return formatted\n",
    "\n",
    "    def remove_links(self, text):\n",
    "        remove_https = re.sub(r'http\\S+', '', text)\n",
    "        remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
    "        return remove_com\n",
    "\n",
    "    def remove_whitespace(self, text):\n",
    "        pattern = re.compile(r'\\s+') \n",
    "        Without_whitespace = re.sub(pattern, ' ', text)\n",
    "        text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "        return text\n",
    "\n",
    "    def accented_characters_removal(self, text):\n",
    "        # Remove accented characters from text using unidecode.\n",
    "        # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "        text = unidecode.unidecode(text)\n",
    "        return text\n",
    "\n",
    "    def lower_casing_text(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def reducing_incorrect_character_repeatation(self, text):\n",
    "        Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "\n",
    "        # Limiting all the  repeatation to two characters.\n",
    "        Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n",
    "\n",
    "        # Pattern matching for all the punctuations that can occur\n",
    "        Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "\n",
    "        # Limiting punctuations in previously formatted string to only one.\n",
    "        Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
    "\n",
    "        # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "        Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "        return Final_Formatted\n",
    "\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        contraction_mapping = self.contraction_map\n",
    "        list_Of_tokens = text.split(' ')\n",
    "        for Word in list_Of_tokens: \n",
    "             if Word in contraction_mapping: \n",
    "                list_Of_tokens = [item.replace(Word, contraction_mapping[Word]) for item in list_Of_tokens]\n",
    "\n",
    "        String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n",
    "        return String_Of_tokens\n",
    "\n",
    "    def removing_special_characters(self, text):\n",
    "        formatted_text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text) \n",
    "        return formatted_text\n",
    "\n",
    "    def remove_numbers(self, text):\n",
    "        formatted_text = re.sub(r\"[^a-zA-Z:$-,%.?!]+\", ' ', text)\n",
    "        return formatted_text\n",
    "\n",
    "    def removing_stopwords(self, text):\n",
    "        text = repr(text)\n",
    "        No_StopWords = [word for word in word_tokenize(text) if word.lower() not in self.stoplist]\n",
    "        words_string = ' '.join(No_StopWords)    \n",
    "        return words_string\n",
    "\n",
    "    def spelling_correction(self, text):\n",
    "        spell = Speller(lang='en')\n",
    "        Corrected_text = spell(text)\n",
    "        return Corrected_text\n",
    "    \n",
    "    def cleaning_and_preprocessing(self, tweet,\n",
    "                              remove_newlines_=True,\n",
    "                              remove_mentions_hashtagSigns_=True,\n",
    "                              strip_html_tags_=True,\n",
    "                              remove_punctuations_=True,\n",
    "                              remove_links_=True,\n",
    "                              remove_whitespace_=True,\n",
    "                              accented_characters_removal_=True,\n",
    "                              lower_casing_text_=True,\n",
    "                              reducing_incorrect_character_repeatation_=True,\n",
    "                              expand_contractions_=True,\n",
    "                              remove_numbers_=True,\n",
    "                              removing_stopwords_=True,\n",
    "                              spelling_correction_=False):\n",
    "    \n",
    "        \"\"\"\n",
    "        input: a single text (e.g., a tweet or a sentence with type string).\n",
    "        output: a single text (e.g., a tweet or a sentence with type string) which is clean:)\n",
    "        \"\"\"\n",
    "\n",
    "        ## Cleaning\n",
    "        text = tweet\n",
    "        if remove_newlines_:\n",
    "            text = self.remove_newlines_tabs(text)\n",
    "        if remove_mentions_hashtagSigns_:\n",
    "            text = self.remove_mentions_hashtagSigns(text)\n",
    "        if strip_html_tags_:\n",
    "            text = self.strip_html_tags(text)\n",
    "        if remove_punctuations_:\n",
    "            text = self.remove_punctuations(text)\n",
    "        if remove_links_:\n",
    "            text = self.remove_links(text)\n",
    "        if remove_whitespace_:\n",
    "            text = self.remove_whitespace(text)\n",
    "        if  accented_characters_removal_:   \n",
    "            text = self.accented_characters_removal(text)\n",
    "        if lower_casing_text_:\n",
    "            text = self.lower_casing_text(text)\n",
    "        if reducing_incorrect_character_repeatation_:\n",
    "            text = self.reducing_incorrect_character_repeatation(text)\n",
    "        if expand_contractions_:\n",
    "            text = self.expand_contractions(text)\n",
    "        if remove_numbers_:\n",
    "            text = self.remove_numbers(text)\n",
    "\n",
    "        ## preprocessing\n",
    "        if removing_stopwords_:\n",
    "            text = self.removing_stopwords(text)\n",
    "        if spelling_correction_:\n",
    "            text = self.spelling_correction(text)\n",
    "        return text\n",
    "\n",
    "    def clean_data(self, df):\n",
    "        \"\"\"\n",
    "        gets whole df as input.\n",
    "        returns df with new column called cleaned tweets.\n",
    "        \"\"\"\n",
    "        cleaned_tweets = []\n",
    "        for tweet in tqdm(df.Text):\n",
    "            cleaned_tweet = self.cleaning_and_preprocessing(tweet)\n",
    "            cleaned_tweets.append(cleaned_tweet)\n",
    "\n",
    "        df[\"cleaned_tweets\"] = cleaned_tweets\n",
    "        return df\n",
    "    \n",
    "    def lemmatize_data(self, df):\n",
    "        \"\"\"\n",
    "        gets whole df as input.\n",
    "        returns df with new column called lemmatized tweets.\n",
    "        \"\"\"\n",
    "        lemmatized_tweets = []\n",
    "        for tweet in tqdm(df.cleaned_tweets):\n",
    "            lemma = [self.lemmatizer.lemmatize(w,'v') for w in self.w_tokenizer.tokenize(tweet)]\n",
    "            lemma = ' '.join(lemma)\n",
    "            lemmatized_tweets.append(lemma)\n",
    "\n",
    "        df[\"lemmatized_tweets\"] = lemmatized_tweets\n",
    "        return df\n",
    "    \n",
    "    def tokenize_data(self, df):\n",
    "        pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "                (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "              | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "              | \\$?\\d+(?:\\.\\d+)?%?\\s?  # currency and percentages, e.g. $12.40, 82%\n",
    "              | \\.\\.\\.              # ellipsis\n",
    "              | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "            '''\n",
    "\n",
    "        pattern = re.compile(pattern)\n",
    "        nltk_tokenized_tweets = [nltk.regexp_tokenize(tweet, pattern)[1:-1] for tweet in df.lemmatized_tweets]\n",
    "        df['Text_words'] = nltk_tokenized_tweets\n",
    "        return df\n",
    "    \n",
    "    def perform_clean_lemmatize_tokenize(self, df):\n",
    "        df = self.clean_data(df)\n",
    "        df = self.lemmatize_data(df)\n",
    "        df = self.tokenize_data(df)\n",
    "        return df\n",
    "    \n",
    "    def clean_query(self, text):\n",
    "        text = self.cleaning_and_preprocessing(text)\n",
    "        lemma = [self.lemmatizer.lemmatize(w,'v') for w in self.w_tokenizer.tokenize(text)]\n",
    "        text = ' '.join(lemma)\n",
    "        pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "                (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "              | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "              | \\$?\\d+(?:\\.\\d+)?%?\\s?  # currency and percentages, e.g. $12.40, 82%\n",
    "              | \\.\\.\\.              # ellipsis\n",
    "              | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "            '''\n",
    "\n",
    "        pattern = re.compile(pattern)\n",
    "        return nltk.regexp_tokenize(text, pattern)[1:-1]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5102397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24116/24116 [00:10<00:00, 2391.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24116/24116 [00:02<00:00, 10876.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Name</th>\n",
       "      <th>Screen Name</th>\n",
       "      <th>UTC</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Language</th>\n",
       "      <th>Client</th>\n",
       "      <th>...</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Media Type</th>\n",
       "      <th>Media URLs</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>lemmatized_tweets</th>\n",
       "      <th>Text_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1541760993121804288</td>\n",
       "      <td>RT @inddais : #inddais is as simple as rocket ...</td>\n",
       "      <td>a happiness seeker.</td>\n",
       "      <td>coucane</td>\n",
       "      <td>2022-06-28T12:30:39.000Z</td>\n",
       "      <td>Tue Jun 28 12:30:39 +0000 2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>video</td>\n",
       "      <td>https://video.twimg.com/ext_tw_video/153998544...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'rt inddais simple rocket science buy seed har...</td>\n",
       "      <td>'rt inddais simple rocket science buy seed har...</td>\n",
       "      <td>[rt, inddais, simple, rocket, science, buy, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1541760992497000453</td>\n",
       "      <td>@ABC How about not leaving pets and kids in th...</td>\n",
       "      <td>p</td>\n",
       "      <td>therealphattyd</td>\n",
       "      <td>2022-06-28T12:30:39.000Z</td>\n",
       "      <td>Tue Jun 28 12:30:39 +0000 2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'how leaving pets kids car start rocket science '</td>\n",
       "      <td>'how leave pet kid car start rocket science '</td>\n",
       "      <td>[how, leave, pet, kid, car, start, rocket, sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1541760990634803202</td>\n",
       "      <td>RT @_demola_ : Science students how farðŸŒš\\n\\nWh...</td>\n",
       "      <td>onobaby</td>\n",
       "      <td>vibequeen01</td>\n",
       "      <td>2022-06-28T12:30:39.000Z</td>\n",
       "      <td>Tue Jun 28 12:30:39 +0000 2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>photo</td>\n",
       "      <td>https://pbs.twimg.com/media/FWUj-FnXkAENk1X.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'rt science students far happen litmus paper a...</td>\n",
       "      <td>'rt science students far happen litmus paper a...</td>\n",
       "      <td>[rt, science, students, far, happen, litmus, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1541760984754405376</td>\n",
       "      <td>RT @DayLabUAB : Wow, congratulations to @DrVic...</td>\n",
       "      <td>Anupam Agarwal</td>\n",
       "      <td>anupamuab</td>\n",
       "      <td>2022-06-28T12:30:37.000Z</td>\n",
       "      <td>Tue Jun 28 12:30:37 +0000 2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'rt wow congratulations new position president...</td>\n",
       "      <td>'rt wow congratulations new position president...</td>\n",
       "      <td>[rt, wow, congratulations, new, position, pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1541760982934237184</td>\n",
       "      <td>RT @mdhn_anbu : Its obvious that studying hist...</td>\n",
       "      <td>Anusha</td>\n",
       "      <td>AnsaMdhn</td>\n",
       "      <td>2022-06-28T12:30:37.000Z</td>\n",
       "      <td>Tue Jun 28 12:30:37 +0000 2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'rt obvious studying history philosophy much b...</td>\n",
       "      <td>'rt obvious study history philosophy much bett...</td>\n",
       "      <td>[rt, obvious, study, history, philosophy, much...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet Id                                               Text  \\\n",
       "0  1541760993121804288  RT @inddais : #inddais is as simple as rocket ...   \n",
       "1  1541760992497000453  @ABC How about not leaving pets and kids in th...   \n",
       "2  1541760990634803202  RT @_demola_ : Science students how farðŸŒš\\n\\nWh...   \n",
       "3  1541760984754405376  RT @DayLabUAB : Wow, congratulations to @DrVic...   \n",
       "4  1541760982934237184  RT @mdhn_anbu : Its obvious that studying hist...   \n",
       "\n",
       "                  Name     Screen Name                       UTC  \\\n",
       "0  a happiness seeker.         coucane  2022-06-28T12:30:39.000Z   \n",
       "1                    p  therealphattyd  2022-06-28T12:30:39.000Z   \n",
       "2              onobaby     vibequeen01  2022-06-28T12:30:39.000Z   \n",
       "3       Anupam Agarwal       anupamuab  2022-06-28T12:30:37.000Z   \n",
       "4               Anusha        AnsaMdhn  2022-06-28T12:30:37.000Z   \n",
       "\n",
       "                       Created At  Favorites  Retweets Language  \\\n",
       "0  Tue Jun 28 12:30:39 +0000 2022          0         0       en   \n",
       "1  Tue Jun 28 12:30:39 +0000 2022          0         0       en   \n",
       "2  Tue Jun 28 12:30:39 +0000 2022          0         0       en   \n",
       "3  Tue Jun 28 12:30:37 +0000 2022          0         0       en   \n",
       "4  Tue Jun 28 12:30:37 +0000 2022          0         0       en   \n",
       "\n",
       "                                              Client  ... Hashtags Mentions  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...  ...        2        0   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...  ...        0        1   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...  ...        0        0   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...  ...        0        1   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...  ...        6        0   \n",
       "\n",
       "   Media Type                                         Media URLs Unnamed: 16  \\\n",
       "0       video  https://video.twimg.com/ext_tw_video/153998544...         NaN   \n",
       "1         NaN                                                NaN         NaN   \n",
       "2       photo    https://pbs.twimg.com/media/FWUj-FnXkAENk1X.jpg         NaN   \n",
       "3         NaN                                                NaN         NaN   \n",
       "4         NaN                                                NaN         NaN   \n",
       "\n",
       "  Unnamed: 17 Unnamed: 18                                     cleaned_tweets  \\\n",
       "0         NaN         NaN  'rt inddais simple rocket science buy seed har...   \n",
       "1         NaN         NaN  'how leaving pets kids car start rocket science '   \n",
       "2         NaN         NaN  'rt science students far happen litmus paper a...   \n",
       "3         NaN         NaN  'rt wow congratulations new position president...   \n",
       "4         NaN         NaN  'rt obvious studying history philosophy much b...   \n",
       "\n",
       "                                   lemmatized_tweets  \\\n",
       "0  'rt inddais simple rocket science buy seed har...   \n",
       "1      'how leave pet kid car start rocket science '   \n",
       "2  'rt science students far happen litmus paper a...   \n",
       "3  'rt wow congratulations new position president...   \n",
       "4  'rt obvious study history philosophy much bett...   \n",
       "\n",
       "                                          Text_words  \n",
       "0  [rt, inddais, simple, rocket, science, buy, se...  \n",
       "1  [how, leave, pet, kid, car, start, rocket, sci...  \n",
       "2  [rt, science, students, far, happen, litmus, p...  \n",
       "3  [rt, wow, congratulations, new, position, pres...  \n",
       "4  [rt, obvious, study, history, philosophy, much...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "df = preprocessor.perform_clean_lemmatize_tokenize(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class TransformerSearch:\n",
    "    def __init__(df):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.data = df\n",
    "        \n",
    "        for tweet in df.Text_words:\n",
    "            tweet_joined = \" \".join(tweet)\n",
    "            sentence_embeddings = model.encode(sentences)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db15f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a49bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
