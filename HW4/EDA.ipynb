{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fa6c9fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/kian/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/kian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import unidecode \n",
    "import pandas as pd \n",
    "import re \n",
    "import time \n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5584a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sentiment140.csv\", header=None, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5f8cd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7b3d056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e97fb311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "1599995    4\n",
       "1599996    4\n",
       "1599997    4\n",
       "1599998    4\n",
       "1599999    4\n",
       "Name: 0, Length: 1600000, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fec40f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['class', 'tweet_id', 'datetime', 'query', 'id', 'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "703e3f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>query</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         class    tweet_id                      datetime     query  \\\n",
       "0            0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "1599995      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996      4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997      4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998      4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999      4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                      id                                              tweet  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "821634e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659775"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "843159a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      "my whole body feels itchy and like its on fire \n",
      "@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
      "@Kwesidei not the whole crew \n",
      "Need a hug \n",
      "@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\n",
      "@Tatiana_K nope they didn't have it \n",
      "@twittera que me muera ? \n",
      "spring break in plain city... it's snowing \n",
      "I just re-pierced my ears \n",
      "@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\n",
      "@octolinz16 It it counts, idk why I did either. you never talk to me anymore \n",
      "@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\n",
      "@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!\n",
      "Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\n",
      "about to file taxes \n",
      "@LettyA ahh ive always wanted to see rent  love the soundtrack!!\n",
      "@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? \n",
      "@alydesigns i was out most of the day so didn't get much done \n",
      "one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh* \n",
      "@angry_barista I baked you a cake but I ated it \n",
      "this week is not going as i had hoped \n",
      "blagh class at 8 tomorrow \n",
      "I hate when I have to call and wake people up \n",
      "Just going to cry myself to sleep after watching Marley and Me.  \n",
      "im sad now  Miss.Lilly\n",
      "ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again \n",
      "Meh... Almost Lover is the exception... this track gets me depressed every time. \n",
      "some1 hacked my account on aim  now i have to make a new one\n",
      "@alielayus I want to go to promote GEAR AND GROOVE but unfornately no ride there  I may b going to the one in Anaheim in May though\n",
      "thought sleeping in was an option tomorrow but realizing that it now is not. evaluations in the morning and work in the afternoon! \n",
      "@julieebaby awe i love you too!!!! 1 am here  i miss you\n",
      "@HumpNinja I cry my asian eyes to sleep at night \n",
      "ok I'm sick and spent an hour sitting in the shower cause I was too sick to stand and held back the puke like a champ. BED now \n",
      "@cocomix04 ill tell ya the story later  not a good day and ill be workin for like three more hours...\n",
      "@MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge\n",
      "@fleurylis I don't either. Its depressing. I don't think I even want to know about the kids in suitcases. \n",
      "Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend \n",
      "really don't feel like getting up today... but got to study to for tomorrows practical exam... \n",
      "He's the reason for the teardrops on my guitar the only one who has enough of me to break my heart \n",
      "Sad, sad, sad. I don't know why but I hate this feeling  I wanna sleep and I still can't!\n",
      "@JonathanRKnight Awww I soo wish I was there to see you finally comfortable! Im sad that I missed it \n",
      "Falling asleep. Just heard about that Tracy girl's body being found. How sad  My heart breaks for that family.\n",
      "@Viennah Yay! I'm happy for you with your job! But that also means less time for me and you... \n",
      "Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?\n",
      "Oh man...was ironing @jeancjumbe's fave top to wear to a meeting. Burnt it \n",
      "is strangely sad about LiLo and SamRo breaking up. \n",
      "@tea oh! i'm so sorry  i didn't think about that before retweeting.\n"
     ]
    }
   ],
   "source": [
    "for t in df.tweet.head(50):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "717a54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = list(df.tweet)\n",
    "tweet_len = list(map(len, tweets_list))\n",
    "df[\"tweet_len\"] = tweet_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73339b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>query</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class    tweet_id                      datetime     query               id  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                               tweet  tweet_len  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...        115  \n",
       "1  is upset that he can't update his Facebook by ...        111  \n",
       "2  @Kenichan I dived many times for the ball. Man...         89  \n",
       "3    my whole body feels itchy and like its on fire          47  \n",
       "4  @nationwideclass no, it's not behaving at all....        111  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb64c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['tweet'].apply(lambda x : x.isascii())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9891d7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].apply(lambda x : x.isascii()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db359060",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Creating moc df for faster dvelepment, remove for the main run #####################\n",
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "47938e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines_tabs(text):\n",
    "    formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ')\n",
    "    formatted_text = formatted_text.replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return formatted_text\n",
    "\n",
    "def remove_mentions_hashtagSigns(text):\n",
    "    tokenized = text.split(\" \")\n",
    "    formatted_text = \"\"\n",
    "    for w in tokenized:\n",
    "        if not w:\n",
    "            continue\n",
    "        if w[0] == \"@\":\n",
    "            continue\n",
    "        if w[0] == \"#\":\n",
    "            formatted_text += f\"{w[1:]} \"\n",
    "        else:\n",
    "            formatted_text += f\"{w} \"\n",
    "            \n",
    "    return formatted_text.strip()\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    formatted = \"\"\n",
    "    for char in text:\n",
    "        if char in string.punctuation:\n",
    "            continue\n",
    "        else:\n",
    "            formatted += char\n",
    "    return formatted\n",
    "\n",
    "def remove_links(text):\n",
    "    remove_https = re.sub(r'http\\S+', '', text)\n",
    "    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
    "    return remove_com\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    pattern = re.compile(r'\\s+') \n",
    "    Without_whitespace = re.sub(pattern, ' ', text)\n",
    "    text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "    return text\n",
    "\n",
    "def accented_characters_removal(text):\n",
    "    # Remove accented characters from text using unidecode.\n",
    "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "def lower_casing_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def reducing_incorrect_character_repeatation(text):\n",
    "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "    \n",
    "    # Limiting all the  repeatation to two characters.\n",
    "    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n",
    "    \n",
    "    # Pattern matching for all the punctuations that can occur\n",
    "    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "    \n",
    "    # Limiting punctuations in previously formatted string to only one.\n",
    "    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
    "    \n",
    "    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "    return Final_Formatted\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    list_Of_tokens = text.split(' ')\n",
    "    for Word in list_Of_tokens: \n",
    "         if Word in contraction_mapping: \n",
    "            list_Of_tokens = [item.replace(Word, contraction_mapping[Word]) for item in list_Of_tokens]\n",
    "                \n",
    "    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n",
    "    return String_Of_tokens\n",
    "\n",
    "def removing_special_characters(text):\n",
    "    formatted_text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text) \n",
    "    return formatted_text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    formatted_text = re.sub(r\"[^a-zA-Z:$-,%.?!]+\", ' ', text)\n",
    "    return formatted_text\n",
    "\n",
    "stoplist = stopwords.words('english') \n",
    "stoplist = set(stoplist)\n",
    "def removing_stopwords(text):\n",
    "    text = repr(text)\n",
    "    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist]\n",
    "    words_string = ' '.join(No_StopWords)    \n",
    "    return words_string\n",
    "\n",
    "def spelling_correction(text):\n",
    "    spell = Speller(lang='en')\n",
    "    Corrected_text = spell(text)\n",
    "    return Corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c3ecbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_and_preprocessing(tweet,\n",
    "                              remove_newlines_=True,\n",
    "                              remove_mentions_hashtagSigns_=True,\n",
    "                              strip_html_tags_=True,\n",
    "                              remove_punctuations_=True,\n",
    "                              remove_links_=True,\n",
    "                              remove_whitespace_=True,\n",
    "                              accented_characters_removal_=True,\n",
    "                              lower_casing_text_=True,\n",
    "                              reducing_incorrect_character_repeatation_=True,\n",
    "                              expand_contractions_=True,\n",
    "                              remove_numbers_=True,\n",
    "                              removing_stopwords_=False,\n",
    "                              spelling_correction_=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    input: a single text (e.g., a tweet or a sentence with type string).\n",
    "    output: a single text (e.g., a tweet or a sentence with type string) which is clean:)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('contraction_map.json') as f:\n",
    "        contraction_map = json.load(f)\n",
    "    \n",
    "    ## Clean\n",
    "    text = tweet\n",
    "    if remove_newlines_:\n",
    "        text = remove_newlines_tabs(text)\n",
    "    if remove_mentions_hashtagSigns_:\n",
    "        text = remove_mentions_hashtagSigns(text)\n",
    "    if strip_html_tags_:\n",
    "        text = strip_html_tags(text)\n",
    "    if remove_punctuations_:\n",
    "        text = remove_punctuations(text)\n",
    "    if remove_links_:\n",
    "        text = remove_links(text)\n",
    "    if remove_whitespace_:\n",
    "        text = remove_whitespace(text)\n",
    "    if  accented_characters_removal_:   \n",
    "        text = accented_characters_removal(text)\n",
    "    if lower_casing_text_:\n",
    "        text = lower_casing_text(text)\n",
    "    if reducing_incorrect_character_repeatation_:\n",
    "        text = reducing_incorrect_character_repeatation(text)\n",
    "    if expand_contractions_:\n",
    "        text = expand_contractions(text, contraction_mapping=contraction_map)\n",
    "    if remove_numbers_:\n",
    "        text = remove_numbers(text)\n",
    "    \n",
    "    ## preprocessing\n",
    "    if removing_stopwords_:\n",
    "        text = removing_stopwords(text)\n",
    "    if spelling_correction_:\n",
    "        text = spelling_correction(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatization(text):\n",
    "    \"\"\"\n",
    "        input: a single string text (e.g., tweet)\n",
    "    \"\"\"\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n",
    "    return lemma    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5e184363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_tweets(df):\n",
    "    \"\"\"\n",
    "        gets whole df as input.\n",
    "        returns df with new column called cleaned tweets.\n",
    "    \"\"\"\n",
    "    cleaned_tweets = []\n",
    "    for tweet in df.tweet:\n",
    "        cleaned_tweet = cleaning_and_preprocessing(tweet)\n",
    "        cleaned_tweets.append(cleaned_tweet)\n",
    "    \n",
    "    df[\"cleaned_tweets\"] = cleaned_tweets\n",
    "    return df\n",
    "\n",
    "\n",
    "def lemmatize_all_tweets(df):\n",
    "    \"\"\"\n",
    "        gets whole df as input.\n",
    "        returns df with new column called lemmatized tweets.\n",
    "    \"\"\"\n",
    "    lemmatized_tweets = []\n",
    "    for tweet in df.cleaned_tweets:\n",
    "        lemmatized_tweet = cleaning_and_preprocessing(tweet)\n",
    "        lemmatized_tweets.append(lemmatized_tweet)\n",
    "    \n",
    "    df[\"lemmatized_tweets\"] = lemmatized_tweets\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6a2ba7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_all_tweets(df)\n",
    "df = lemmatize_all_tweets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "61fc58f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>query</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_len</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>lemmatized_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>115</td>\n",
       "      <td>aww that is a bummer you shoulda got david ca...</td>\n",
       "      <td>aww that is a bummer you shoulda got david car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>111</td>\n",
       "      <td>is upset that he cannot update his facebook by...</td>\n",
       "      <td>is upset that he cannot update his facebook by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>89</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>47</td>\n",
       "      <td>my whole body feels itchy and like it is on fire</td>\n",
       "      <td>my whole body feels itchy and like it is on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>111</td>\n",
       "      <td>no it is not behaving at all i am mad why am i...</td>\n",
       "      <td>no it is not behaving at all i am mad why am i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class    tweet_id                      datetime     query               id  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                               tweet  tweet_len  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...        115   \n",
       "1  is upset that he can't update his Facebook by ...        111   \n",
       "2  @Kenichan I dived many times for the ball. Man...         89   \n",
       "3    my whole body feels itchy and like its on fire          47   \n",
       "4  @nationwideclass no, it's not behaving at all....        111   \n",
       "\n",
       "                                      cleaned_tweets  \\\n",
       "0   aww that is a bummer you shoulda got david ca...   \n",
       "1  is upset that he cannot update his facebook by...   \n",
       "2  i dived many times for the ball managed to sav...   \n",
       "3   my whole body feels itchy and like it is on fire   \n",
       "4  no it is not behaving at all i am mad why am i...   \n",
       "\n",
       "                                   lemmatized_tweets  \n",
       "0  aww that is a bummer you shoulda got david car...  \n",
       "1  is upset that he cannot update his facebook by...  \n",
       "2  i dived many times for the ball managed to sav...  \n",
       "3   my whole body feels itchy and like it is on fire  \n",
       "4  no it is not behaving at all i am mad why am i...  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ce5e6145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     aww that is a bummer you shoulda got david car...\n",
       "1     is upset that he cannot update his facebook by...\n",
       "2     i dived many times for the ball managed to sav...\n",
       "3      my whole body feels itchy and like it is on fire\n",
       "4     no it is not behaving at all i am mad why am i...\n",
       "                            ...                        \n",
       "95                       strider is a sick little puppy\n",
       "96    so ryleegracewana go steves party or not sadly...\n",
       "97    hey i actually won one of my bracket pools too...\n",
       "98       you do not follow me either and i work for you\n",
       "99    a bad nite for the favorite teams astros and s...\n",
       "Name: lemmatized_tweets, Length: 100, dtype: object"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lemmatized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04d594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "26f61b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?\\s?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "\n",
    "pattern = re.compile(pattern)\n",
    "\n",
    "final_tokenized_tweets = [nltk.regexp_tokenize(tweet, pattern)[1:-1] for tweet in df.lemmatized_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "531e2d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['that',\n",
       "  'is',\n",
       "  'a',\n",
       "  'bummer',\n",
       "  'you',\n",
       "  'shoulda',\n",
       "  'got',\n",
       "  'david',\n",
       "  'carr',\n",
       "  'of',\n",
       "  'third',\n",
       "  'day',\n",
       "  'to',\n",
       "  'do',\n",
       "  'it'],\n",
       " ['upset',\n",
       "  'that',\n",
       "  'he',\n",
       "  'cannot',\n",
       "  'update',\n",
       "  'his',\n",
       "  'facebook',\n",
       "  'by',\n",
       "  'texting',\n",
       "  'it',\n",
       "  'and',\n",
       "  'might',\n",
       "  'cry',\n",
       "  'as',\n",
       "  'a',\n",
       "  'result',\n",
       "  'school',\n",
       "  'today',\n",
       "  'also'],\n",
       " ['dived',\n",
       "  'many',\n",
       "  'times',\n",
       "  'for',\n",
       "  'the',\n",
       "  'ball',\n",
       "  'managed',\n",
       "  'to',\n",
       "  'save',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'go',\n",
       "  'out',\n",
       "  'of'],\n",
       " ['whole', 'body', 'feels', 'itchy', 'and', 'like', 'it', 'is', 'on'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'not',\n",
       "  'behaving',\n",
       "  'at',\n",
       "  'all',\n",
       "  'i',\n",
       "  'am',\n",
       "  'mad',\n",
       "  'why',\n",
       "  'am',\n",
       "  'i',\n",
       "  'here',\n",
       "  'because',\n",
       "  'i',\n",
       "  'cannot',\n",
       "  'see',\n",
       "  'you',\n",
       "  'all',\n",
       "  'over'],\n",
       " ['the', 'whole'],\n",
       " ['a'],\n",
       " ['long',\n",
       "  'ti',\n",
       "  'ame',\n",
       "  'no',\n",
       "  'see',\n",
       "  'yes',\n",
       "  'rains',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'only',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'lol',\n",
       "  'i',\n",
       "  'am',\n",
       "  'fine',\n",
       "  'thanks',\n",
       "  'how',\n",
       "  'is'],\n",
       " ['they', 'did', 'not', 'have'],\n",
       " ['me'],\n",
       " ['break', 'in', 'plain', 'city', 'it', 'is'],\n",
       " ['just', 'repierced', 'my'],\n",
       " ['could',\n",
       "  'not',\n",
       "  'bear',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'it',\n",
       "  'and',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'the',\n",
       "  'ua',\n",
       "  'loss',\n",
       "  'was'],\n",
       " ['it',\n",
       "  'counts',\n",
       "  'idk',\n",
       "  'why',\n",
       "  'i',\n",
       "  'did',\n",
       "  'either',\n",
       "  'you',\n",
       "  'never',\n",
       "  'talk',\n",
       "  'to',\n",
       "  'me'],\n",
       " ['would',\n",
       "  'have',\n",
       "  'been',\n",
       "  'the',\n",
       "  'first',\n",
       "  'but',\n",
       "  'i',\n",
       "  'did',\n",
       "  'not',\n",
       "  'have',\n",
       "  'a',\n",
       "  'gun',\n",
       "  'not',\n",
       "  'really',\n",
       "  'though',\n",
       "  'zac',\n",
       "  'snyders',\n",
       "  'just',\n",
       "  'a'],\n",
       " ['wish',\n",
       "  'i',\n",
       "  'got',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'it',\n",
       "  'with',\n",
       "  'you',\n",
       "  'i',\n",
       "  'miss',\n",
       "  'you',\n",
       "  'and',\n",
       "  'how',\n",
       "  'was',\n",
       "  'the'],\n",
       " ['death',\n",
       "  'scene',\n",
       "  'will',\n",
       "  'hurt',\n",
       "  'me',\n",
       "  'severely',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'on',\n",
       "  'film',\n",
       "  'wry',\n",
       "  'is',\n",
       "  'directors',\n",
       "  'cut',\n",
       "  'not',\n",
       "  'out'],\n",
       " ['to', 'file'],\n",
       " ['i', 'have', 'always', 'wanted', 'to', 'see', 'rent', 'love', 'the'],\n",
       " ['dear',\n",
       "  'we',\n",
       "  'are',\n",
       "  'you',\n",
       "  'drinking',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'forgotten',\n",
       "  'table'],\n",
       " ['was', 'out', 'most', 'of', 'the', 'day', 'so', 'did', 'not', 'get', 'much'],\n",
       " ['of',\n",
       "  'my',\n",
       "  'friend',\n",
       "  'called',\n",
       "  'me',\n",
       "  'and',\n",
       "  'asked',\n",
       "  'to',\n",
       "  'meet',\n",
       "  'with',\n",
       "  'her',\n",
       "  'at',\n",
       "  'mid',\n",
       "  'valley',\n",
       "  'todaybut',\n",
       "  'i',\n",
       "  'have',\n",
       "  'no',\n",
       "  'time'],\n",
       " ['baked', 'you', 'a', 'cake', 'but', 'i', 'ated'],\n",
       " ['week', 'is', 'not', 'going', 'as', 'i', 'had'],\n",
       " ['class', 'at'],\n",
       " ['hate', 'when', 'i', 'have', 'to', 'call', 'and', 'wake', 'people'],\n",
       " ['going',\n",
       "  'to',\n",
       "  'cry',\n",
       "  'myself',\n",
       "  'to',\n",
       "  'sleep',\n",
       "  'after',\n",
       "  'watching',\n",
       "  'marley',\n",
       "  'and'],\n",
       " ['am', 'sad', 'now'],\n",
       " ['lol',\n",
       "  'that',\n",
       "  'leslie',\n",
       "  'and',\n",
       "  'ok',\n",
       "  'i',\n",
       "  'will',\n",
       "  'not',\n",
       "  'do',\n",
       "  'it',\n",
       "  'again',\n",
       "  'so',\n",
       "  'leslie',\n",
       "  'will',\n",
       "  'not',\n",
       "  'get',\n",
       "  'mad'],\n",
       " ['almost',\n",
       "  'lover',\n",
       "  'is',\n",
       "  'the',\n",
       "  'exception',\n",
       "  'this',\n",
       "  'track',\n",
       "  'gets',\n",
       "  'me',\n",
       "  'depressed',\n",
       "  'every'],\n",
       " ['hacked',\n",
       "  'my',\n",
       "  'account',\n",
       "  'on',\n",
       "  'aim',\n",
       "  'now',\n",
       "  'i',\n",
       "  'have',\n",
       "  'to',\n",
       "  'make',\n",
       "  'a',\n",
       "  'new'],\n",
       " ['want',\n",
       "  'to',\n",
       "  'go',\n",
       "  'to',\n",
       "  'promote',\n",
       "  'gear',\n",
       "  'and',\n",
       "  'groove',\n",
       "  'but',\n",
       "  'unfornately',\n",
       "  'no',\n",
       "  'ride',\n",
       "  'there',\n",
       "  'i',\n",
       "  'may',\n",
       "  'b',\n",
       "  'going',\n",
       "  'to',\n",
       "  'the',\n",
       "  'one',\n",
       "  'in',\n",
       "  'anaheim',\n",
       "  'in',\n",
       "  'may'],\n",
       " ['sleeping',\n",
       "  'in',\n",
       "  'was',\n",
       "  'an',\n",
       "  'option',\n",
       "  'tomorrow',\n",
       "  'but',\n",
       "  'realizing',\n",
       "  'that',\n",
       "  'it',\n",
       "  'now',\n",
       "  'is',\n",
       "  'not',\n",
       "  'evaluations',\n",
       "  'in',\n",
       "  'the',\n",
       "  'morning',\n",
       "  'and',\n",
       "  'work',\n",
       "  'in',\n",
       "  'the'],\n",
       " ['i', 'love', 'you', 'too', 'am', 'here', 'i', 'miss'],\n",
       " ['cry', 'my', 'asian', 'eyes', 'to', 'sleep', 'at'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'sick',\n",
       "  'and',\n",
       "  'spent',\n",
       "  'an',\n",
       "  'hour',\n",
       "  'sitting',\n",
       "  'in',\n",
       "  'the',\n",
       "  'shower',\n",
       "  'because',\n",
       "  'i',\n",
       "  'was',\n",
       "  'too',\n",
       "  'sick',\n",
       "  'to',\n",
       "  'stand',\n",
       "  'and',\n",
       "  'held',\n",
       "  'back',\n",
       "  'the',\n",
       "  'puke',\n",
       "  'like',\n",
       "  'a',\n",
       "  'champ',\n",
       "  'bed'],\n",
       " ['wi',\n",
       "  'will',\n",
       "  'tell',\n",
       "  'ya',\n",
       "  'the',\n",
       "  'story',\n",
       "  'later',\n",
       "  'not',\n",
       "  'a',\n",
       "  'good',\n",
       "  'day',\n",
       "  'and',\n",
       "  'i',\n",
       "  'wi',\n",
       "  'will',\n",
       "  'be',\n",
       "  'workin',\n",
       "  'for',\n",
       "  'like',\n",
       "  'three',\n",
       "  'more'],\n",
       " ['bed', 'time', 'came', 'here'],\n",
       " ['do',\n",
       "  'not',\n",
       "  'either',\n",
       "  'it',\n",
       "  'is',\n",
       "  'depressing',\n",
       "  'i',\n",
       "  'do',\n",
       "  'not',\n",
       "  'think',\n",
       "  'i',\n",
       "  'even',\n",
       "  'want',\n",
       "  'to',\n",
       "  'know',\n",
       "  'about',\n",
       "  'the',\n",
       "  'kids',\n",
       "  'in'],\n",
       " ['class',\n",
       "  'work',\n",
       "  'gym',\n",
       "  'or',\n",
       "  'then',\n",
       "  'class',\n",
       "  'another',\n",
       "  'day',\n",
       "  'that',\n",
       "  'is',\n",
       "  'gonna',\n",
       "  'fly',\n",
       "  'by',\n",
       "  'i',\n",
       "  'miss',\n",
       "  'my'],\n",
       " ['do',\n",
       "  'not',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'getting',\n",
       "  'up',\n",
       "  'today',\n",
       "  'but',\n",
       "  'got',\n",
       "  'to',\n",
       "  'study',\n",
       "  'to',\n",
       "  'for',\n",
       "  'tomorrows',\n",
       "  'practical'],\n",
       " ['is',\n",
       "  'the',\n",
       "  'reason',\n",
       "  'for',\n",
       "  'the',\n",
       "  'teardrops',\n",
       "  'on',\n",
       "  'my',\n",
       "  'guitar',\n",
       "  'the',\n",
       "  'only',\n",
       "  'one',\n",
       "  'who',\n",
       "  'has',\n",
       "  'enough',\n",
       "  'of',\n",
       "  'me',\n",
       "  'to',\n",
       "  'break',\n",
       "  'my'],\n",
       " ['sad',\n",
       "  'sad',\n",
       "  'i',\n",
       "  'do',\n",
       "  'not',\n",
       "  'know',\n",
       "  'why',\n",
       "  'but',\n",
       "  'i',\n",
       "  'hate',\n",
       "  'this',\n",
       "  'feeling',\n",
       "  'i',\n",
       "  'wanna',\n",
       "  'sleep',\n",
       "  'and',\n",
       "  'i',\n",
       "  'still'],\n",
       " ['i',\n",
       "  'soo',\n",
       "  'wish',\n",
       "  'i',\n",
       "  'was',\n",
       "  'there',\n",
       "  'to',\n",
       "  'see',\n",
       "  'you',\n",
       "  'finally',\n",
       "  'comfortable',\n",
       "  'i',\n",
       "  'am',\n",
       "  'sad',\n",
       "  'that',\n",
       "  'i',\n",
       "  'missed'],\n",
       " ['asleep',\n",
       "  'just',\n",
       "  'heard',\n",
       "  'about',\n",
       "  'that',\n",
       "  'tracy',\n",
       "  'girls',\n",
       "  'body',\n",
       "  'being',\n",
       "  'found',\n",
       "  'how',\n",
       "  'sad',\n",
       "  'my',\n",
       "  'heart',\n",
       "  'breaks',\n",
       "  'for',\n",
       "  'that'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'happy',\n",
       "  'for',\n",
       "  'you',\n",
       "  'with',\n",
       "  'your',\n",
       "  'job',\n",
       "  'but',\n",
       "  'that',\n",
       "  'also',\n",
       "  'means',\n",
       "  'less',\n",
       "  'ti',\n",
       "  'ame',\n",
       "  'for',\n",
       "  'me',\n",
       "  'and'],\n",
       " ['checked',\n",
       "  'my',\n",
       "  'user',\n",
       "  'timeline',\n",
       "  'on',\n",
       "  'my',\n",
       "  'blackberry',\n",
       "  'it',\n",
       "  'looks',\n",
       "  'like',\n",
       "  'the',\n",
       "  'twanking',\n",
       "  'is',\n",
       "  'still',\n",
       "  'happening',\n",
       "  'are',\n",
       "  'ppl',\n",
       "  'still',\n",
       "  'having',\n",
       "  'probs',\n",
       "  'w',\n",
       "  'bgs',\n",
       "  'and'],\n",
       " ['manwas',\n",
       "  'ironing',\n",
       "  'fave',\n",
       "  'top',\n",
       "  'to',\n",
       "  'wear',\n",
       "  'to',\n",
       "  'a',\n",
       "  'meeting',\n",
       "  'burnt'],\n",
       " ['strangely', 'sad', 'about', 'lilo', 'and', 'samro', 'breaking'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'sorry',\n",
       "  'i',\n",
       "  'did',\n",
       "  'not',\n",
       "  'think',\n",
       "  'about',\n",
       "  'that',\n",
       "  'before'],\n",
       " ['plan',\n",
       "  'a',\n",
       "  'massive',\n",
       "  'broken',\n",
       "  'promise',\n",
       "  'via',\n",
       "  'wwdiigocomtautao',\n",
       "  'still',\n",
       "  'waiting',\n",
       "  'for',\n",
       "  'broadband',\n",
       "  'we'],\n",
       " ['tons',\n",
       "  'of',\n",
       "  'replies',\n",
       "  'from',\n",
       "  'you',\n",
       "  'may',\n",
       "  'have',\n",
       "  'to',\n",
       "  'unfollow',\n",
       "  'so',\n",
       "  'i',\n",
       "  'can',\n",
       "  'see',\n",
       "  'my',\n",
       "  'friends',\n",
       "  'tweets',\n",
       "  'you',\n",
       "  'are',\n",
       "  'scrolling',\n",
       "  'the',\n",
       "  'feed',\n",
       "  'a'],\n",
       " ['duck', 'and', 'chicken', 'are', 'taking', 'wayy', 'too', 'long', 'to'],\n",
       " ['vacation',\n",
       "  'photos',\n",
       "  'online',\n",
       "  'a',\n",
       "  'few',\n",
       "  'yrs',\n",
       "  'ago',\n",
       "  'pc',\n",
       "  'crashed',\n",
       "  'and',\n",
       "  'now',\n",
       "  'i',\n",
       "  'forget',\n",
       "  'the',\n",
       "  'name',\n",
       "  'of',\n",
       "  'the'],\n",
       " ['need', 'a'],\n",
       " ['sure',\n",
       "  'what',\n",
       "  'they',\n",
       "  'are',\n",
       "  'only',\n",
       "  'that',\n",
       "  'they',\n",
       "  'are',\n",
       "  'pos',\n",
       "  'as',\n",
       "  'much',\n",
       "  'as',\n",
       "  'i',\n",
       "  'want',\n",
       "  'to',\n",
       "  'i',\n",
       "  'do',\n",
       "  'not',\n",
       "  'think',\n",
       "  'can',\n",
       "  'trade',\n",
       "  'away',\n",
       "  'company',\n",
       "  'assets',\n",
       "  'sorry'],\n",
       " ['hate', 'when', 'that'],\n",
       " ['have',\n",
       "  'a',\n",
       "  'sad',\n",
       "  'feeling',\n",
       "  'that',\n",
       "  'dallas',\n",
       "  'is',\n",
       "  'not',\n",
       "  'going',\n",
       "  'to',\n",
       "  'show',\n",
       "  'up',\n",
       "  'i',\n",
       "  'gotta',\n",
       "  'say',\n",
       "  'though',\n",
       "  'you',\n",
       "  'would',\n",
       "  'think',\n",
       "  'more',\n",
       "  'shows',\n",
       "  'would',\n",
       "  'use',\n",
       "  'music',\n",
       "  'from',\n",
       "  'the',\n",
       "  'game'],\n",
       " ['degrees'],\n",
       " ['did',\n",
       "  'u',\n",
       "  'move',\n",
       "  'to',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'u',\n",
       "  'we',\n",
       "  'are',\n",
       "  'already',\n",
       "  'in',\n",
       "  'sd',\n",
       "  'hmm',\n",
       "  'random',\n",
       "  'u',\n",
       "  'found',\n",
       "  'me',\n",
       "  'glad',\n",
       "  'to',\n",
       "  'hear',\n",
       "  'yer',\n",
       "  'doing',\n",
       "  'we'],\n",
       " ['miss',\n",
       "  'my',\n",
       "  'ps',\n",
       "  'it',\n",
       "  'is',\n",
       "  'out',\n",
       "  'of',\n",
       "  'commission',\n",
       "  'wutcha',\n",
       "  'playing',\n",
       "  'have',\n",
       "  'you',\n",
       "  'copped',\n",
       "  'blood',\n",
       "  'on',\n",
       "  'the'],\n",
       " ['leaving', 'the', 'parking', 'lot', 'of'],\n",
       " ['life', 'is', 'cool', 'but', 'not', 'for'],\n",
       " ['though',\n",
       "  'i',\n",
       "  'have',\n",
       "  'never',\n",
       "  'gotten',\n",
       "  'to',\n",
       "  'experience',\n",
       "  'the',\n",
       "  'post',\n",
       "  'coitus',\n",
       "  'cigarette',\n",
       "  'before',\n",
       "  'and',\n",
       "  'now',\n",
       "  'i',\n",
       "  'never'],\n",
       " ['had',\n",
       "  'such',\n",
       "  'a',\n",
       "  'nice',\n",
       "  'day',\n",
       "  'too',\n",
       "  'bad',\n",
       "  'the',\n",
       "  'rain',\n",
       "  'comes',\n",
       "  'in',\n",
       "  'tomorrow',\n",
       "  'at'],\n",
       " ['bad',\n",
       "  'i',\n",
       "  'will',\n",
       "  'not',\n",
       "  'be',\n",
       "  'around',\n",
       "  'i',\n",
       "  'lost',\n",
       "  'my',\n",
       "  'job',\n",
       "  'and',\n",
       "  'cannot',\n",
       "  'even',\n",
       "  'pay',\n",
       "  'my',\n",
       "  'phone',\n",
       "  'bill',\n",
       "  'lmao',\n",
       "  'aw'],\n",
       " ['back', 'to', 'school'],\n",
       " ['jobs',\n",
       "  'no',\n",
       "  'money',\n",
       "  'how',\n",
       "  'in',\n",
       "  'the',\n",
       "  'he',\n",
       "  'will',\n",
       "  'is',\n",
       "  'min',\n",
       "  'wage',\n",
       "  'here',\n",
       "  'fn',\n",
       "  'clams',\n",
       "  'an'],\n",
       " ['forever', 'see', 'you'],\n",
       " ['i', 'saw', 'the', 'failwhale', 'all', 'day'],\n",
       " ['haha',\n",
       "  'dude',\n",
       "  'i',\n",
       "  'do',\n",
       "  'not',\n",
       "  'really',\n",
       "  'look',\n",
       "  'at',\n",
       "  'em',\n",
       "  'unless',\n",
       "  'someone',\n",
       "  'says',\n",
       "  'hey',\n",
       "  'i',\n",
       "  'added',\n",
       "  'you',\n",
       "  'sorry',\n",
       "  'i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'terrible',\n",
       "  'at',\n",
       "  'that',\n",
       "  'i',\n",
       "  'need',\n",
       "  'a',\n",
       "  'pop'],\n",
       " ['am',\n",
       "  'sure',\n",
       "  'you',\n",
       "  'are',\n",
       "  'right',\n",
       "  'i',\n",
       "  'need',\n",
       "  'to',\n",
       "  'start',\n",
       "  'working',\n",
       "  'out',\n",
       "  'with',\n",
       "  'you',\n",
       "  'and',\n",
       "  'the',\n",
       "  'nikster',\n",
       "  'or',\n",
       "  'jared',\n",
       "  'at'],\n",
       " ['really',\n",
       "  'hate',\n",
       "  'how',\n",
       "  'people',\n",
       "  'diss',\n",
       "  'my',\n",
       "  'bands',\n",
       "  'trace',\n",
       "  'is',\n",
       "  'clearly',\n",
       "  'not'],\n",
       " ['attire',\n",
       "  'today',\n",
       "  'was',\n",
       "  'puma',\n",
       "  'singlet',\n",
       "  'adidas',\n",
       "  'shortsand',\n",
       "  'black',\n",
       "  'business',\n",
       "  'socks',\n",
       "  'and',\n",
       "  'leather',\n",
       "  'shoes',\n",
       "  'lucky',\n",
       "  'did',\n",
       "  'not',\n",
       "  'run',\n",
       "  'into',\n",
       "  'any',\n",
       "  'cute'],\n",
       " ['will', 'not', 'you', 'show', 'my'],\n",
       " ['picnic', 'my', 'phone', 'smells', 'like'],\n",
       " ['donkey',\n",
       "  'is',\n",
       "  'sensitive',\n",
       "  'about',\n",
       "  'such',\n",
       "  'comments',\n",
       "  'nevertheless',\n",
       "  'he',\n",
       "  'would',\n",
       "  'and',\n",
       "  'med',\n",
       "  'be',\n",
       "  'glad',\n",
       "  'to',\n",
       "  'see',\n",
       "  'your',\n",
       "  'mug',\n",
       "  'asap',\n",
       "  'charger',\n",
       "  'is',\n",
       "  'still'],\n",
       " ['new', 'csi', 'tonight'],\n",
       " ['think', 'my', 'arms', 'are', 'sore', 'from'],\n",
       " ['why',\n",
       "  'someone',\n",
       "  'that',\n",
       "  'u',\n",
       "  'like',\n",
       "  'so',\n",
       "  'much',\n",
       "  'can',\n",
       "  'make',\n",
       "  'you',\n",
       "  'so',\n",
       "  'unhappy',\n",
       "  'in',\n",
       "  'a',\n",
       "  'split',\n",
       "  'seccond'],\n",
       " ['soon',\n",
       "  'i',\n",
       "  'just',\n",
       "  'hate',\n",
       "  'saying',\n",
       "  'bye',\n",
       "  'and',\n",
       "  'see',\n",
       "  'you',\n",
       "  'tomorrow',\n",
       "  'for',\n",
       "  'the'],\n",
       " ['got',\n",
       "  'ur',\n",
       "  'newsletter',\n",
       "  'those',\n",
       "  'fares',\n",
       "  'really',\n",
       "  'are',\n",
       "  'unbelievable',\n",
       "  'shame',\n",
       "  'i',\n",
       "  'already',\n",
       "  'booked',\n",
       "  'and',\n",
       "  'paid',\n",
       "  'for'],\n",
       " ['the'],\n",
       " ['too'],\n",
       " ['i', 'do', 'not', 'have', 'any', 'chalk', 'my', 'chalkboard', 'is'],\n",
       " ['a',\n",
       "  'blast',\n",
       "  'at',\n",
       "  'the',\n",
       "  'getty',\n",
       "  'villa',\n",
       "  'but',\n",
       "  'hates',\n",
       "  'that',\n",
       "  'she',\n",
       "  'is',\n",
       "  'had',\n",
       "  'a',\n",
       "  'sore',\n",
       "  'throat',\n",
       "  'all',\n",
       "  'day',\n",
       "  'it',\n",
       "  'is',\n",
       "  'just',\n",
       "  'getting',\n",
       "  'worse'],\n",
       " ['missed', 'ya', 'at', 'the', 'meeting', 'sup'],\n",
       " ['tummy',\n",
       "  'hurts',\n",
       "  'i',\n",
       "  'wonder',\n",
       "  'if',\n",
       "  'the',\n",
       "  'hypnosis',\n",
       "  'has',\n",
       "  'anything',\n",
       "  'to',\n",
       "  'do',\n",
       "  'with',\n",
       "  'it',\n",
       "  'if',\n",
       "  'so',\n",
       "  'it',\n",
       "  'is',\n",
       "  'working',\n",
       "  'i',\n",
       "  'get',\n",
       "  'it',\n",
       "  'stop'],\n",
       " ['is', 'it', 'always', 'the', 'fat'],\n",
       " ['babe',\n",
       "  'my',\n",
       "  'fam',\n",
       "  'annoys',\n",
       "  'me',\n",
       "  'too',\n",
       "  'thankfully',\n",
       "  'they',\n",
       "  'are',\n",
       "  'asleep',\n",
       "  'right',\n",
       "  'now',\n",
       "  'muahaha',\n",
       "  'evil'],\n",
       " ['should',\n",
       "  'have',\n",
       "  'paid',\n",
       "  'more',\n",
       "  'attention',\n",
       "  'when',\n",
       "  'we',\n",
       "  'covered',\n",
       "  'photoshop',\n",
       "  'in',\n",
       "  'my',\n",
       "  'webpage',\n",
       "  'design',\n",
       "  'class',\n",
       "  'in'],\n",
       " ['my', 'bday', 'do', 'not', 'know', 'what'],\n",
       " ['cameron', 'the'],\n",
       " ['for',\n",
       "  'me',\n",
       "  'please',\n",
       "  'the',\n",
       "  'ex',\n",
       "  'is',\n",
       "  'threatening',\n",
       "  'to',\n",
       "  'start',\n",
       "  'sh',\n",
       "  'at',\n",
       "  'myour',\n",
       "  'babies',\n",
       "  'st',\n",
       "  'birthday',\n",
       "  'party',\n",
       "  'what',\n",
       "  'a',\n",
       "  'jerk',\n",
       "  'and',\n",
       "  'i',\n",
       "  'still',\n",
       "  'have',\n",
       "  'a'],\n",
       " ['do',\n",
       "  'u',\n",
       "  'really',\n",
       "  'enjoy',\n",
       "  'being',\n",
       "  'with',\n",
       "  'him',\n",
       "  'if',\n",
       "  'the',\n",
       "  'problems',\n",
       "  'are',\n",
       "  'too',\n",
       "  'constants',\n",
       "  'u',\n",
       "  'should',\n",
       "  'think',\n",
       "  'things',\n",
       "  'more',\n",
       "  'find',\n",
       "  'someone'],\n",
       " ['is', 'a', 'sick', 'little'],\n",
       " ['ryleegracewana',\n",
       "  'go',\n",
       "  'steves',\n",
       "  'party',\n",
       "  'or',\n",
       "  'not',\n",
       "  'sadly',\n",
       "  'since',\n",
       "  'it',\n",
       "  'is',\n",
       "  'easter',\n",
       "  'i',\n",
       "  'wnt',\n",
       "  'b',\n",
       "  'able',\n",
       "  'do',\n",
       "  'much',\n",
       "  'but',\n",
       "  'ohh',\n",
       "  'we'],\n",
       " ['i',\n",
       "  'actually',\n",
       "  'won',\n",
       "  'one',\n",
       "  'of',\n",
       "  'my',\n",
       "  'bracket',\n",
       "  'pools',\n",
       "  'too',\n",
       "  'bad',\n",
       "  'it',\n",
       "  'was',\n",
       "  'not',\n",
       "  'the',\n",
       "  'one',\n",
       "  'for'],\n",
       " ['do', 'not', 'follow', 'me', 'either', 'and', 'i', 'work', 'for'],\n",
       " ['bad',\n",
       "  'nite',\n",
       "  'for',\n",
       "  'the',\n",
       "  'favorite',\n",
       "  'teams',\n",
       "  'astros',\n",
       "  'and',\n",
       "  'spartans',\n",
       "  'lose',\n",
       "  'the',\n",
       "  'nite',\n",
       "  'out',\n",
       "  'with',\n",
       "  'tw',\n",
       "  'was']]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tokenized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c2bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
